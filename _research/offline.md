---
title: "Offline Learning"
layout: single-portfolio
excerpt: "<img src='/images/research/offline.png' alt=''>"
collection: research
order_number: 10
header: 
  og_image: "research/offline.png"
---

In this research I study the property of offline learning. Combining conservative Q learning with Double DQN actor-critic structure, also I add a regularizer from SAC model,
I have a derivation to show that the model can still get a conservative bound comparing to traditional Double DQN, assuming the uncertainty follows a Gaussian Distribution with fixed variance.
And I use a recommender system problem to the effectiveness of this method.
## Article

Dingrong Wang et al. "Conservative Evidential Learning of Long-Term User Preferences" *In Submission*.

[//]: # (>)
[//]: # ([Article]&#40;https://doi.org/10.1177/07388942211015242&#41;{: .btn--research} [Preprint]&#40;/files/pdf/research/Turning the Lights on.pdf&#41;{: .btn--research} [Supplemental Information]&#40;/files/pdf/research/Turning the Lights on SI.pdf&#41;{: .btn--research} [Replication Archive]&#40;https://journals.sagepub.com/doi/suppl/10.1177/07388942211015242&#41;{: .btn--research} [GitHub Repo]&#40;https://github.com/jayrobwilliams/conflict-preemption&#41;{: .btn--research} [Poster]&#40;/files/pdf/research/PSS 2018 Poster.pdf&#41;{: .btn--research})
